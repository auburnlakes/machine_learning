{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>About this Exercise</h1>\n",
    "\n",
    "This tutorial will provide you with enough information to finish CIS537 projects that use [PyTorch](https://pytorch.org/) but it is not meant to be a comprehensive tutorial for PyTorch. For a complete tutorial on Pytorch, review PyTorch's official tutorial:\n",
    "https://pytorch.org/tutorials/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>What is PyTorch?</h3>\n",
    "\n",
    "PyTorch is a python-based deep learning platform that provides GPU support. To start, first import the PyTorch packages and some of its some packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import PyTorch and its subpackages\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "# Also other packages for convenience\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "sys.path.append('/home/codio/workspace/.modules')\n",
    "import helper as h"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>0. Tensors</h3>\n",
    "\n",
    "Tensors are the basic building blocks of PyTorch. They are similar to NumPy's ndarrays. For example, to create a tensor of all zeros with size (3, 2), you can do the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.]])\n",
      "torch.Size([3, 2])\n"
     ]
    }
   ],
   "source": [
    "X = torch.zeros(3, 2) # this syntax is similar to NumPy. In Numpy, we would do np.zeros((3,2))\n",
    "\n",
    "print(X)\n",
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also convert a NumPy array to a torch Tensor: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NumPy Array:  [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14]\n",
      "Pytorch Tensor:  tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14])\n"
     ]
    }
   ],
   "source": [
    "X_numpy = np.arange(15)\n",
    "print('NumPy Array: ', X_numpy)\n",
    "\n",
    "X_tensor = torch.tensor(X_numpy)\n",
    "print('Pytorch Tensor: ', X_tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also convert a torch Tensor to a NumPy array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14]\n",
      "<class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "X_tensor_numpy = X_tensor.numpy()\n",
    "\n",
    "print(X_tensor_numpy)\n",
    "print(type(X_tensor_numpy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just like NumPy, you can do arithmetic operations on torch Tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Addition in Numpy [2 7 9]\n",
      "Addition in PyTorch tensor([2., 7., 9.])\n",
      "\n",
      "Scalar Multiplication in Numpy [3 6 9]\n",
      "Scalar Multiplication in PyTorch tensor([3., 6., 9.])\n",
      "\n",
      "Elementwise Multiplication in Numpy [ 1 10 18]\n",
      "Elementiwse Multiplication in PyTorch tensor([ 1., 10., 18.])\n",
      "\n",
      "Matrix Multiplication in Numpy 29\n",
      "Matrix Multiplication in PyTorch tensor(29.)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create two numpy arrays\n",
    "A = np.array([1, 2, 3])\n",
    "B = np.array([1, 5, 6])\n",
    "\n",
    "# Convert the two numpy arrays into torch Tensor\n",
    "A_tensor = torch.Tensor(A)\n",
    "B_tensor = torch.Tensor(B)\n",
    "\n",
    "# addition / subtraction\n",
    "print('Addition in Numpy', A + B)\n",
    "print('Addition in PyTorch', A_tensor + B_tensor)\n",
    "print()\n",
    "\n",
    "# scalar multiplication\n",
    "print('Scalar Multiplication in Numpy', 3*A)\n",
    "print('Scalar Multiplication in PyTorch', 3*A_tensor)\n",
    "print()\n",
    "\n",
    "# elementwise multiplication\n",
    "print('Elementwise Multiplication in Numpy', A*B)\n",
    "print('Elementiwse Multiplication in PyTorch', A_tensor*B_tensor)\n",
    "print()\n",
    "\n",
    "# matrix multiplication\n",
    "# this is slightly different from NumPy\n",
    "print('Matrix Multiplication in Numpy', A@B)\n",
    "print('Matrix Multiplication in PyTorch', torch.matmul(A_tensor, B_tensor))\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some other useful tensor operations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elementwise Comparison in NumPy:  [ True False False]\n",
      "Elementwise omparison in PyTorch:  tensor([ True, False, False])\n",
      "\n",
      "C [[10  9  8]\n",
      " [ 6  7  5]\n",
      " [ 1  2  3]]\n",
      "\n",
      "Sum along row (down) in NumPy:  [17 18 16]\n",
      "Sum along row (down) in PyTorch:  tensor([17., 18., 16.])\n",
      "\n",
      "Mean along column (across) in NumPy:  [9. 6. 2.]\n",
      "Mean along column (across) in PyTorch:  tensor([9., 6., 2.])\n",
      "\n",
      "argmax along column (across) in NumPy:  [0 1 2]\n",
      "argmax along column (across) in PyTorch:  tensor([0, 1, 2])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Elementwise comparison\n",
    "print('Elementwise Comparison in NumPy: ', A == B)\n",
    "print('Elementwise omparison in PyTorch: ', A_tensor == B_tensor)\n",
    "print()\n",
    "\n",
    "# Generate a random matrix\n",
    "C = np.array([[10, 9, 8], [6, 7, 5], [1, 2, 3]])\n",
    "C_tensor = torch.Tensor(C)\n",
    "\n",
    "print('C', C)\n",
    "print()\n",
    "\n",
    "# Sum along the row\n",
    "# In NumPy, we specify the axis.\n",
    "# In PyTorch, we specify the dim\n",
    "print('Sum along row (down) in NumPy: ', np.sum(C, axis=0))\n",
    "print('Sum along row (down) in PyTorch: ' ,torch.sum(C_tensor, dim=0))\n",
    "print()\n",
    "\n",
    "# Find the mean along the column\n",
    "# In NumPy, we specify the axis.\n",
    "# In PyTorch, we specify the dim\n",
    "print('Mean along column (across) in NumPy: ', np.mean(C, axis=1))\n",
    "print('Mean along column (across) in PyTorch: ', torch.mean(C_tensor, dim=1))\n",
    "print()\n",
    "\n",
    "\n",
    "# Find the argmax along the column\n",
    "# In NumPy, we specify the axis.\n",
    "# In PyTorch, we specify the dim\n",
    "print('argmax along column (across) in NumPy: ', np.argmax(C, axis=1))\n",
    "print('argmax along column (across) in PyTorch: ', torch.argmax(C_tensor, dim=1))\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>1. Creating a model</h3>\n",
    "\n",
    "In the following sections, we are going to develop a multilayer perceptron for our favorite spiral dataset. To start, let's generate and visualize the dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Running the following code with generate\n",
    "# 600 datapoints\n",
    "# 300 in class 0 \n",
    "# 300 in class 1\n",
    "X, y = h.spiraldata(N=300)\n",
    "\n",
    "h.visualize_2D(X.numpy(), y.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we will create a multilayer perceptron. In Pytorch, to create a model, we need to create a model class that overloads <code>nn.Module</code> and implement the <code>forward()</code> function that performs forward propagation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a model class named MLP\n",
    "# overloads nn.Module \n",
    "\n",
    "# This MLP is a simple multilayer perceptron with 3-hidden layers \n",
    "# with ReLU transition function\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        # call the constructor of the inherited module\n",
    "        # this line is always necessary\n",
    "        super().__init__()\n",
    "        \n",
    "        # All the layers with trainable parameters have to be \n",
    "        # defined in the constructor\n",
    "        # Create a linear layer that takes input of dimension 2\n",
    "        # and outputs a vector of dimension 64\n",
    "        # Similarly, create the consequent layers\n",
    "        self.fc1 = nn.Linear(in_features=2, out_features=64)\n",
    "        self.fc2 = nn.Linear(in_features=64, out_features=32)\n",
    "        self.fc3 = nn.Linear(in_features=32, out_features=16)\n",
    "        \n",
    "        # The last layer should output a two dimensional vector\n",
    "        # where the first dimension corresponds to the network's\n",
    "        # belief that the input belongs to class 0\n",
    "        # and similarly, second dimension corresponds to class 1\n",
    "        self.fc4 = nn.Linear(in_features=16, out_features=2)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # apply the first linear transformation to input x\n",
    "        # then apply the ReLU transition to get the output\n",
    "        # of the first hidden layer o1\n",
    "        # Similar procedures are done to get o2, o3\n",
    "        o1 = F.relu(self.fc1(x))\n",
    "        o2 = F.relu(self.fc2(o1))\n",
    "        o3 = F.relu(self.fc3(o2))\n",
    "        \n",
    "        # We do not apply any transition function to the output\n",
    "        # of the final hidden layer\n",
    "        o4 = self.fc4(o3)\n",
    "        return o4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can instantiate a model and do forward propagation by running the following cell: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([600, 2])\n"
     ]
    }
   ],
   "source": [
    "# Instantiate a model\n",
    "model = MLP()\n",
    "\n",
    "# Run forward propagation\n",
    "logits = model(X)\n",
    "\n",
    "# Print the output of the model\n",
    "# This should be (600, 2)\n",
    "print(logits.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>2. Define a loss function</h3>\n",
    "\n",
    "To train a neural network, we need a loss function. Since we are doing a classification problem, we are going to use the cross entropy loss. Luckily, Pytorch has implemented this for us. For a comprehensive list of PyTorch implemented loss function, please check out: https://pytorch.org/docs/1.1.0/nn.html#loss-functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the cross entropy loss function\n",
    "# this loss function takes in logits and labels and return the loss value\n",
    "# Concretely, loss_fn(logits, y) returns the cross entropy loss between\n",
    "# the prediction and ground truth\n",
    "loss_fn = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>3. Define an optimizer </h3>\n",
    "We are going to use an SGD optimizer to optimize our MLP. Conveniently, PyTorch has implemented SGD for us. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.optim.SGD is the optimizer that PyTorch implemented\n",
    "# it takes the model's parameter, which you can get by calling\n",
    "# model.parameters()\n",
    "# and learning rate lr\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>4. Training the Network </h3>\n",
    "\n",
    "With all the ingredients we have, we are ready to train our multilayer perceptrons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Num_epochs is the number we are going to go through the dataset\n",
    "# This is a simple dataset so we are just going to do a vanilla Gradient Descent\n",
    "# rather than minibatch Stochastic Gradient Descent\n",
    "num_epochs = 2000\n",
    "\n",
    "losses = []\n",
    "for epoch in range(num_epochs):\n",
    "    # Set the model to training mode\n",
    "    # This step is always necessary during training \n",
    "    model.train()\n",
    "    \n",
    "    # PyTorch keeps track of all the gradients generated \n",
    "    # throughout the whole training procedure\n",
    "    # But we only need the gradient of the current time step\n",
    "    # so we need to zero-out the gradient buffer\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    # Do forward propagation\n",
    "    logits = model(X)\n",
    "    \n",
    "    # Calculate the loss \n",
    "    loss = loss_fn(logits, y)\n",
    "    \n",
    "    # Do backward propagation\n",
    "    loss.backward()\n",
    "    \n",
    "    # Update the model parameters \n",
    "    optimizer.step()\n",
    "    \n",
    "    # Get the prediction\n",
    "    prediction = torch.argmax(logits, dim=1)\n",
    "    \n",
    "    # PyTorch supports elements elementwise comparison\n",
    "    acc = torch.mean((prediction == y).float())\n",
    "    \n",
    "    losses.append(loss.item())\n",
    "    \n",
    "    # print the loss value every 50 epochs\n",
    "    if (epoch + 1) % 50 == 0:\n",
    "        print('Epoch [{}/ {}], Loss: {:.4f} '.format(epoch + 1, num_epochs, losses[-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the training loss\n",
    "plt.plot(range(1, num_epochs + 1), losses, label='Training Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>5. Evaluate the Network </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Need to go into eval mode\n",
    "model.eval()\n",
    "\n",
    "# Do a forward pass and tell PyTorch to not keep track of\n",
    "# the gradient calculation to save memory\n",
    "with torch.no_grad():\n",
    "    logits = model(X)\n",
    "\n",
    "# make prediction\n",
    "prediction = torch.argmax(logits, dim=1)\n",
    "\n",
    "# Calculate the accracuy\n",
    "print('Accuracy: {:.2f}%'.format(torch.mean((prediction == y).float()).item() * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize the model\n",
    "h.visclassifier(model, X.numpy(), y.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>(Optional) Exercise</h3>\n",
    "\n",
    "In the following cell, you will create another multilayer perceptron with 2 hidden layers. The first hidden layer has 256 neurons and the second hidden layer has 128 neurons. We have provided the following code stub. What you need to is to fill in the blanks. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP_v2(nn.Module):\n",
    "    def __init__(self):\n",
    "        # call the constructor of the inherited module\n",
    "        # this line is always necessary\n",
    "        super().__init__()\n",
    "        \n",
    "        # TODO: fill in the blank with the right layer\n",
    "        self.fc1 = None\n",
    "        self.fc2 = None\n",
    "        self.fc3 = None\n",
    "        \n",
    "        ### BEGIN SOLUTION\n",
    "        self.fc1 = nn.Linear(in_features=2, out_features=256)\n",
    "        self.fc2 = nn.Linear(in_features=256, out_features=128)\n",
    "        self.fc3 = nn.Linear(in_features=128, out_features=2)\n",
    "        ### END SOLUTION\n",
    "        \n",
    "    def forward(self, x):\n",
    "        o1 = F.relu(self.fc1(x))\n",
    "        o2 = F.relu(self.fc2(o1))\n",
    "        o3 = self.fc3(o2)\n",
    "        return o3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_v2 = MLP_v2()\n",
    "\n",
    "# If you create the right model, you should be able to pass the following test\n",
    "\n",
    "# Checking the size of the parameters in the first layer\n",
    "fc1_parameters = list(model_v2.fc1.parameters())\n",
    "\n",
    "# the weight should have size (256, 2)\n",
    "assert fc1_parameters[0].shape == (256, 2)\n",
    "# the bias should have size (256, )\n",
    "assert fc1_parameters[1].shape == (256, )\n",
    "\n",
    "# Checking the size of the parameters in the second layer\n",
    "fc2_parameters = list(model_v2.fc2.parameters())\n",
    "assert fc2_parameters[0].shape == (128, 256)\n",
    "assert fc2_parameters[1].shape == (128, )\n",
    "\n",
    "# Checking the size of the parameters in the last layer\n",
    "fc3_parameters = list(model_v2.fc3.parameters())\n",
    "assert fc3_parameters[0].shape == (2, 128)\n",
    "assert fc3_parameters[1].shape == (2, )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating a new optimizer\n",
    "# Feel free to play around with the learning rate\n",
    "optimizer_v2 = torch.optim.SGD(model_v2.parameters(), lr=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "num_epochs = 2000\n",
    "\n",
    "losses = []\n",
    "for epoch in range(num_epochs):\n",
    "    # Set the model to training mode\n",
    "    # This step is always necessary during training \n",
    "    model_v2.train()\n",
    "    \n",
    "    # PyTorch keeps track of all the gradients generated \n",
    "    # throughout the whole training procedure\n",
    "    # But we only need the gradient of the current time step\n",
    "    # so we need to zero-out the gradient buffer\n",
    "    optimizer_v2.zero_grad()\n",
    "    \n",
    "    # Do forward propagation\n",
    "    logits = model_v2(X)\n",
    "    \n",
    "    # Calculate the loss \n",
    "    loss = loss_fn(logits, y)\n",
    "    \n",
    "    # Do backward propagation\n",
    "    loss.backward()\n",
    "    \n",
    "    # Update the model parameters \n",
    "    optimizer_v2.step()\n",
    "    \n",
    "    # Get the prediction\n",
    "    prediction = torch.argmax(logits, dim=1)\n",
    "    \n",
    "    # PyTorch supports elements elementwise comparison\n",
    "    acc = torch.mean((prediction == y).float())\n",
    "    \n",
    "    losses.append(loss.item())\n",
    "    \n",
    "    # print the loss value every 50 epochs\n",
    "    if (epoch + 1) % 50 == 0:\n",
    "        print('Epoch [{}/ {}], Loss: {:.4f} '.format(epoch + 1, num_epochs, losses[-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model\n",
    "# Need to go into eval mode\n",
    "model_v2.eval()\n",
    "\n",
    "# Do a forward pass and tell PyTorch to not keep track of\n",
    "# the gradient calculation to save memory\n",
    "with torch.no_grad():\n",
    "    logits = model_v2(X)\n",
    "\n",
    "# make prediction\n",
    "prediction = torch.argmax(logits, dim=1)\n",
    "\n",
    "# Calculate the accracuy\n",
    "print('Accuracy: {:.2f}%'.format(torch.mean((prediction == y).float()).item() * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize the model\n",
    "h.visclassifier(model_v2, X.numpy(), y.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tensorflow approach\n",
    "\n",
    "With Tensorflow, you can follow the simple 5 steps to train your model on data\n",
    "\n",
    "Like in Pytorch, first you define your network with layers. However, unlike in Pytorch, the model definition in Tensorflow also defines the exact forward propagation. In other words, Tensorflow automatically attaches the layers for you whereas Pytorch requires you to define a `forward` method for forward propagation. The latter approach has both an upside and a downside: gives the user more flexibility but demands repetitive work.\n",
    "\n",
    "With Tensorflow's `compile` and `fit` methods, you can easily and cleanly perform training. Again, Pytorch provides the user more flexibility by requiring them to write the training/evaluation step in a Python-ic way. On the other hand, Tensorflow provides in-built functions that conveniently connect to `compile` and `fit`. \n",
    "\n",
    "[Tensorflow's recent versions provide more control on the training step like Pytorch.](https://www.tensorflow.org/tutorials/quickstart/advanced)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the v1 model in Tensorflow\n",
    "\n",
    "1. Creating a model\n",
    "2. Define a loss function\n",
    "3. Define an optimizer\n",
    "4. Training the Network\n",
    "\n",
    "Notes:\n",
    "- We use the `SparseCategoricalCrossentropy` loss function since the true `y` is a vector of are binary labels of shape `len(data)` and the logit outputs of the model is a matrix of logits of shape `len(data), 2`. Since the outputs are not normalized between 0 and 1, we specify `from_logits=True`.\n",
    "- By default, Tensorflow outputs loss and metrics at the end of every epoch. We use a custom logger function to only output every 50 epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_np = X.numpy()\n",
    "y_np = y.numpy()\n",
    "\n",
    "## Creating a model\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.Input(shape=(2)),\n",
    "    tf.keras.layers.Dense(64, activation='relu', name='fc1'),\n",
    "    tf.keras.layers.Dense(32, activation='relu', name='fc2'),\n",
    "    tf.keras.layers.Dense(16, activation='relu', name='fc3'),\n",
    "    tf.keras.layers.Dense(2, activation=None, name='fc4'),\n",
    "])\n",
    "\n",
    "# Display the model's parameters\n",
    "model.summary()\n",
    "\n",
    "## Define a loss function\n",
    "loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "\n",
    "## Define an optimizer\n",
    "optimizer = tf.keras.optimizers.SGD(learning_rate=0.1)\n",
    "\n",
    "## Training the network\n",
    "model.compile(optimizer=optimizer, loss=loss_fn, metrics=['accuracy'])\n",
    "history = model.fit(\n",
    "    X_np,\n",
    "    y_np,\n",
    "    epochs=num_epochs,\n",
    "    verbose=0, # disable the default progress bar\n",
    "    callbacks=[h.ProgBarLoggerNEpochs(num_epochs, every_n=50)],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`model.summary()` is an incredibly useful tool. It also shows the number of parameters per layer. \n",
    "\n",
    "For example, since the first layer is $64$ units and the input dimension is $2$, the first weight matrix $\\mathbf{W}_1$ must be of shape $2 \\times 64$. Recall that $\\mathbf{Z}_1 = \\sigma(\\mathbf{W}_1 \\mathbf{x} + \\mathbf{b})$ where $\\mathbf{x}$ is one input point and $\\mathbf{b}$ is the bias vector added after matrix multiplication of $\\mathbf{W}_1$ with $\\mathbf{x}$. To match dimensions, the vectors $\\mathbf{b}$ and $\\mathbf{W}_1 \\mathbf{x}$ must be of equal length, i.e., $64$. Hence, how many parameters does the first layer have in total?\n",
    "\n",
    "Well, it is $2 \\times 64 + 64 = 192$, which we see in the summary table. Likewise, the second layer has $64 \\times 32 + 32 = 2080$ parameters, and so on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_losses = history.history['loss']\n",
    "\n",
    "plt.plot(range(1, num_epochs + 1), training_losses, label='Training Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Evaluate the Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Evaluate the Network\n",
    "model.evaluate(X_np, y_np)\n",
    "\n",
    "## OR\n",
    "print('Calculating accuracy in Tensorflow manually with logits')\n",
    "\n",
    "# get logits\n",
    "logits = model(X_np)\n",
    "\n",
    "# make prediction\n",
    "prediction = tf.argmax(logits, axis=1)\n",
    "\n",
    "# Calculate the accuracy\n",
    "accuracy = tf.reduce_mean(tf.cast(prediction == y, tf.float32)).numpy()\n",
    "print('Accuracy: {:.2f}%'.format(accuracy * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize the model\n",
    "h.visclassifier_tf(model, X_np, y_np)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
